# -*- coding: utf-8 -*-
"""Pretrained Emotional Analysis live final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eKbgEvcjLY4gmFOaMQ_jVfKySbQvvl72

# Emotional Analysis from live reaction

Install DeepFace popularly used for face recognition and identification tasks.

Using DeepFace as it is lightweight face recognition and attribute analysis library.

DeepFace systems have an impressive accuracy of 97%, FaceNet is more successful but for this task 97% accuracy is enough.

DeepFace has many pre-trained models for face detection and recognition.
"""

pip install deepface opencv-python library #Installing DeepFace a deep learning face recognition system that identifies human faces in digital images.

"""Import relevant dependencies"""

# import dependencies
from deepface import DeepFace#Importing deepface popular for face recognition and classification tasks
from IPython.display import display, Javascript, Image, clear_output#To display the output, use javascript and clear it
from google.colab.output import eval_js#For teh use of JS code in python
from base64 import b64decode, b64encode#Decodes base 64 to bytes and vice versa
import cv2#Used for loading and processing images
import numpy as np#Used when working with arrays
import PIL#For use of images
import io#Manage file related input and outputs
import html#HyperTextMarkupLanguage used to structure and create web pages.
import time#Time elapsed

"""Create a function to convert JS object to CV image"""

# function to convert the JavaScript object into an OpenCV image
def js_to_image(js_reply):
  """
  Params:
          js_reply: JavaScript object containing image from webcam
  Returns:
          img: OpenCV BGR image
  """
  # decode base64 image
  image_bytes = b64decode(js_reply.split(',')[1])  # Split the base64 string and decode the image part
  # convert bytes to numpy array
  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)  # Convert the decoded bytes into a numpy array
  # decode numpy array into OpenCV BGR image
  img = cv2.imdecode(jpg_as_np, flags=1)  # Decode the numpy array into an OpenCV BGR image

  return img  # Return the OpenCV image

# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream
def bbox_to_bytes(bbox_array):
  """
  Params:
          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.
  Returns:
        bytes: Base64 image byte string
  """
  # convert array into PIL image
  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')  # Convert the numpy array into a PIL image with RGBA format
  iobuf = io.BytesIO()  # Create a BytesIO object to hold the image data
  # format bbox into png for return
  bbox_PIL.save(iobuf, format='png')  # Save the PIL image to the BytesIO object in PNG format
  # format return string
  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))  # Encode the image data in base64 and format it as a data URL

  return bbox_bytes  # Return the base64 image byte string

"""Initalize model"""

# initialize the Haar Cascade face detection model
face_cascade = cv2.CascadeClassifier(  # Create a CascadeClassifier object
    cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')  # Load the Haar Cascade model file
)

"""Function to take photo from webcam"""

# function to take a photo using the webcam, detect faces, draw bounding boxes, and save the image
def take_photo(filename='photo.jpg', quality=0.8):
  js = Javascript('''  // JavaScript code to capture a photo using the webcam
    async function takePhoto(quality) {
      const div = document.createElement('div');  // Create a div element
      const capture = document.createElement('button');  // Create a button element
      capture.textContent = 'Capture';  // Set button text to 'Capture'
      div.appendChild(capture);  // Append button to the div

      const video = document.createElement('video');  // Create a video element
      video.style.display = 'block';  // Set video display to block
      const stream = await navigator.mediaDevices.getUserMedia({video: true});  // Get video stream from webcam

      document.body.appendChild(div);  // Append div to the document body
      div.appendChild(video);  // Append video to the div
      video.srcObject = stream;  // Set video source to webcam stream
      await video.play();  // Play the video

      // Resize the output to fit the video element.
      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);  // Adjust iframe height to fit the video element

      // Wait for Capture button to be clicked.
      await new Promise((resolve) => capture.onclick = resolve);  // Wait for Capture button click

      const canvas = document.createElement('canvas');  // Create a canvas element
      canvas.width = video.videoWidth;  // Set canvas width to video width
      canvas.height = video.videoHeight;  // Set canvas height to video height
      canvas.getContext('2d').drawImage(video, 0, 0);  // Draw the current video frame on the canvas
      stream.getVideoTracks()[0].stop();  // Stop the video stream
      div.remove();  // Remove the div element
      return canvas.toDataURL('image/jpeg', quality);  // Return the image data as a base64 string
    }
    ''')
  display(js)  # Display the JavaScript in the notebook

  # get photo data
  data = eval_js('takePhoto({})'.format(quality))  # Execute the JavaScript to take a photo and get the data URL
  # get OpenCV format image
  img = js_to_image(data)  # Convert the data URL to an OpenCV image
  # grayscale img
  gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)  # Convert the image to grayscale
  print(gray.shape)  # Print the shape of the grayscale image
  # get face bounding box coordinates using Haar Cascade
  faces = face_cascade.detectMultiScale(gray)  # Detect faces in the grayscale image using Haar Cascade
  # draw face bounding box on image
  for (x,y,w,h) in faces:  # Loop through each detected face
      img = cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)  # Draw a rectangle around the face
  # save image
  cv2.imwrite(filename, img)  # Save the image with bounding boxes to the specified file

  return filename  # Return the filename of the saved image

"""Decided to use a pretrained model as was unable to train the model on the large dataset I had and limited RAM did attempt using Pycharm and VS Code aswell but this did not help much with the issue.

Take in live stream from webcam
"""

# JavaScript to properly create our live video stream using our webcam as input
def video_stream():
  js = Javascript('''  // JavaScript code to create a video stream
    var video;  // Variable to hold the video element
    var div = null;  // Variable to hold the div element
    var stream;  // Variable to hold the video stream
    var captureCanvas;  // Variable to hold the canvas element for capturing frames
    var imgElement;  // Variable to hold the image element
    var labelElement;  // Variable to hold the label element

    var pendingResolve = null;  // Variable to hold the pending resolve function
    var shutdown = false;  // Variable to track if the stream should be shut down

    function removeDom() {  // Function to remove the DOM elements and stop the stream
       stream.getVideoTracks()[0].stop();  // Stop the video stream
       video.remove();  // Remove the video element
       div.remove();  // Remove the div element
       video = null;  // Reset the video variable
       div = null;  // Reset the div variable
       stream = null;  // Reset the stream variable
       imgElement = null;  // Reset the imgElement variable
       captureCanvas = null;  // Reset the captureCanvas variable
       labelElement = null;  // Reset the labelElement variable
    }

    function onAnimationFrame() {  // Function to handle animation frames
      if (!shutdown) {  // If not shutting down
        window.requestAnimationFrame(onAnimationFrame);  // Request the next animation frame
      }
      if (pendingResolve) {  // If there is a pending resolve function
        var result = "";  // Initialize result as an empty string
        if (!shutdown) {  // If not shutting down
          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);  // Draw the video frame on the canvas
          result = captureCanvas.toDataURL('image/jpeg', 0.8);  // Get the image data as a base64 string
        }
        var lp = pendingResolve;  // Store the pending resolve function
        pendingResolve = null;  // Reset the pending resolve function
        lp(result);  // Resolve with the result
      }
    }

    async function createDom() {  // Function to create the DOM elements
      if (div !== null) {  // If the div already exists
        return stream;  // Return the existing stream
      }

      div = document.createElement('div');  // Create a div element
      div.style.border = '2px solid black';  // Set the border style
      div.style.padding = '3px';  // Set the padding
      div.style.width = '100%';  // Set the width
      div.style.maxWidth = '600px';  // Set the max width
      document.body.appendChild(div);  // Append the div to the document body

      const modelOut = document.createElement('div');  // Create a div element for model output
      modelOut.innerHTML = "Status:";  // Set the inner HTML
      labelElement = document.createElement('span');  // Create a span element for the label
      labelElement.innerText = 'No data';  // Set the initial label text
      labelElement.style.fontWeight = 'bold';  // Set the font weight
      modelOut.appendChild(labelElement);  // Append the label to the model output div
      div.appendChild(modelOut);  // Append the model output div to the main div

      video = document.createElement('video');  // Create a video element
      video.style.display = 'block';  // Set the display style
      video.width = div.clientWidth - 6;  // Set the width of the video
      video.setAttribute('playsinline', '');  // Set the playsinline attribute
      video.onclick = () => { shutdown = true; };  // Set the onclick handler to shut down
      stream = await navigator.mediaDevices.getUserMedia(  // Get the video stream from the webcam
          {video: { facingMode: "environment"}});  // Set the facing mode to environment (rear camera)
      div.appendChild(video);  // Append the video to the div

      imgElement = document.createElement('img');  // Create an image element
      imgElement.style.position = 'absolute';  // Set the position style
      imgElement.style.zIndex = 1;  // Set the z-index
      imgElement.onclick = () => { shutdown = true; };  // Set the onclick handler to shut down
      div.appendChild(imgElement);  // Append the image to the div

      const instruction = document.createElement('div');  // Create a div element for instructions
      instruction.innerHTML =
          '' +
          'When finished, click here or on the video to stop this demo';  // Set the instructions text
      div.appendChild(instruction);  // Append the instruction div to the main div
      instruction.onclick = () => { shutdown = true; };  // Set the onclick handler to shut down

      video.srcObject = stream;  // Set the video source to the stream
      await video.play();  // Play the video

      captureCanvas = document.createElement('canvas');  // Create a canvas element
      captureCanvas.width = 640;  // Set the canvas width
      captureCanvas.height = 480;  // Set the canvas height
      window.requestAnimationFrame(onAnimationFrame);  // Request the first animation frame

      return stream;  // Return the stream
    }

    async function stream_frame(label, imgData) {  // Function to stream a frame
      if (shutdown) {  // If shutting down
        removeDom();  // Remove the DOM elements
        shutdown = false;  // Reset the shutdown flag
        return '';  // Return an empty string
      }

      var preCreate = Date.now();  // Get the current time before creating the DOM
      stream = await createDom();  // Create the DOM elements and get the stream

      var preShow = Date.now();  // Get the current time before showing the label
      if (label != "") {  // If there is a label
        labelElement.innerHTML = label;  // Update the label element
      }

      if (imgData != "") {  // If there is image data
        var videoRect = video.getClientRects()[0];  // Get the bounding rectangle of the video element
        imgElement.style.top = videoRect.top + "px";  // Set the top position of the image
        imgElement.style.left = videoRect.left + "px";  // Set the left position of the image
        imgElement.style.width = videoRect.width + "px";  // Set the width of the image
        imgElement.style.height = videoRect.height + "px";  // Set the height of the image
        imgElement.src = imgData;  // Set the source of the image to the image data
      }

      var preCapture = Date.now();  // Get the current time before capturing the frame
      var result = await new Promise(function(resolve, reject) {  // Create a promise to capture the frame
        pendingResolve = resolve;  // Set the pending resolve function
      });
      shutdown = false;  // Reset the shutdown flag

      return {'create': preShow - preCreate,  // Return the timing data and the captured image
              'show': preCapture - preShow,
              'capture': Date.now() - preCapture,
              'img': result};
    }
    ''')

  display(js)  # Display the JavaScript code in the notebook

def video_frame(label, bbox):
  data = eval_js('stream_frame("{}", "{}")'.format(label, bbox))  # Execute the JavaScript function with the label and bbox
  return data  # Return the data

"""Capture the frame"""

# Function to capture a frame from the video stream and convert it to an OpenCV image
def capture_frame():
    js = Javascript('''  // JavaScript code to capture a frame from the video stream
        async function captureFrame() {
            const video = document.querySelector('video');  // Select the video element on the page
            const canvas = document.createElement('canvas');  // Create a canvas element
            canvas.width = video.videoWidth;  // Set the canvas width to the video width
            canvas.height = video.videoHeight;  // Set the canvas height to the video height
            canvas.getContext('2d').drawImage(video, 0, 0);  // Draw the current video frame onto the canvas
            return canvas.toDataURL('image/jpeg', 0.8);  // Return the image data as a base64 string
        }
    ''')
    display(js)  # Display the JavaScript code in the notebook
    data = eval_js('captureFrame()')  # Execute the JavaScript function to capture a frame
    return js_to_image(data)  # Convert the base64 image data to an OpenCV image and return it

"""Show results of the bounding box and label"""

# start streaming video from webcam
video_stream()
# label for video
label_html = 'Capturing...'
# initialize bounding box to empty
bbox = ''
count = 0

# Start an infinite loop to process video frames
while True:
    js_reply = video_frame(label_html, bbox)  # Get the current frame and bounding box from the video stream
    if not js_reply:  # If no frame is returned, break the loop
        break

    # convert JS response to OpenCV Image
    img = js_to_image(js_reply["img"])  # Convert the JavaScript response to an OpenCV image

    # create transparent overlay for bounding box
    bbox_array = np.zeros([480, 640, 4], dtype=np.uint8)  # Create an empty array for the bounding box overlay

    # grayscale image for face detection
    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)  # Convert the image to grayscale

    # get face region coordinates
    faces = face_cascade.detectMultiScale(gray)  # Detect faces in the grayscale image
    # get face bounding box for overlay
    for (x, y, w, h) in faces:  # Loop through each detected face
        bbox_array = cv2.rectangle(bbox_array, (x, y), (x + w, y + h), (255, 0, 0), 2)  # Draw a rectangle around the face

    bbox_array[:, :, 3] = (bbox_array.max(axis=2) > 0).astype(int) * 255  # Set the alpha channel for the bounding box overlay
    # convert overlay of bbox into bytes
    bbox_bytes = bbox_to_bytes(bbox_array)  # Convert the bounding box array to a base64 byte string
    # update bbox so next frame gets new overlay
    bbox = bbox_bytes  # Update the bounding box for the next frame

    # capture a frame from the video stream
    frame = capture_frame()
    # analyze the captured frame for emotions
    result = DeepFace.analyze(frame, actions=['emotion'])
    dominant_emotion = result[0]['dominant_emotion']  # Get the dominant emotion from the analysis

    clear_output(wait=True)  # Clear the output in the notebook
    print("Emotion displayed is", dominant_emotion)  # Print the detected emotion

    # Introduce a small delay to avoid overwhelming the system
    time.sleep(1)  # Wait for 1 second before processing the next frame
